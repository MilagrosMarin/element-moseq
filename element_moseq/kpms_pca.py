import datajoint as dj
import matplotlib.pyplot as plt
import cv2
from typing import Optional
import numpy as np
from datetime import datetime

import inspect
import importlib
import os
from pathlib import Path
from element_interface.utils import find_full_path, dict_to_uuid

schema = dj.schema()
_linking_module = None


def activate(
    pca_schema_name: str,
    *,
    create_schema: bool = True,
    create_tables: bool = True,
    linking_module: str = None,
):
    """Activate this schema.

    Args:
        pca_schema_name (str): A string containing the name of the pca schema.
        create_schema (bool): If True (default), schema  will be created in the database.
        create_tables (bool): If True (default), tables related to the schema will be created in the database.
        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.

    Dependencies:
    Functions:
        get_kpms_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s)
        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder.
    """

    if isinstance(linking_module, str):
        linking_module = importlib.import_module(linking_module)
    assert inspect.ismodule(
        linking_module
    ), "The argument 'dependency' must be a module's name or a module"

    assert hasattr(
        linking_module, "get_kpms_root_data_dir"
    ), "The linking module must specify a lookup function for a root data directory"

    global _linking_module
    _linking_module = linking_module

    # activate
    schema.activate(
        pca_schema_name,
        create_schema=create_schema,
        create_tables=create_tables,
        add_objects=_linking_module.__dict__,
    )


# -------------- Functions required by the element-moseq ---------------


def get_kpms_root_data_dir() -> list:
    """Pulls relevant func from parent namespace to specify root data dir(s).

    It is recommended that all paths in DataJoint Elements stored as relative
    paths, with respect to some user-configured "root" director(y/ies). The
    root(s) may vary between data modalities and user machines. Returns a full path
    string or list of strings for possible root data directories.
    """
    root_directories = _linking_module.get_kpms_root_data_dir()
    if isinstance(root_directories, (str, Path)):
        root_directories = [root_directories]

    if (
        hasattr(_linking_module, "get_kpms_processed_data_dir")
        and get_kpms_processed_data_dir() not in root_directories
    ):
        root_directories.append(_linking_module.get_kpms_processed_data_dir())

    return root_directories


def get_kpms_processed_data_dir() -> Optional[str]:
    """Pulls relevant func from parent namespace. Defaults to KPMS's project /videos/.

    Method in parent namespace should provide a string to a directory where KPMS output
    files will be stored. If unspecified, output files will be stored in the
    session directory 'videos' folder, per DeepLabCut default.
    """
    if hasattr(_linking_module, "get_kpms_processed_data_dir"):
        return _linking_module.get_kpms_processed_data_dir()
    else:
        return None


# ----------------------------- Table declarations ----------------------

@schema
class PoseEstimationMethod(dj.Lookup):
    """Table for storing the pose estimation method used to obtain the keypoints data.

    Attributes:
        format (str)                : Pose estimation method.
        pose_estimation_desc (str)  : Pose estimation method description.
    """

    definition = """ 
    # Parameters used to obtain the keypoints data based on a specific pose estimation method.        
    format                          : char(15)         # deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap,
    ---
    pose_estimation_desc            : varchar(1000)    # Optional. Pose estimation method description
    """

    contents = [
        ["deeplabcut", "`.csv` and `.h5/.hdf5` files generated by DeepLabcut analysis"],
        ["sleap", "`.slp` and `.h5/.hdf5` files generated by SLEAP analysis"],
        ["anipose", "`.csv` files generated by anipose analysis"],
        ["sleap-anipose", "`.h5/.hdf5` files generated by sleap-anipose analysis"],
        ["nwb", "`.nwb` files with Neurodata Without Borders (NWB) format"],
        ["facemap", "`.h5` files generated by Facemap analysis"],
    ]


@schema
class KeypointSet(dj.Manual):
    """Table for storing the keypoint sets and their associated videos.

    Attributes:
        kpset_id (int): Unique ID for each keypoint set.
        kpset_config_path (str): Path relative to root data directory where the config file is located.
        kpset_videos_path (str): Path relative to root data directory where the videos and their keypoints are located.
        kpset_description (str): Optional. User-entered description.
    """

    definition = """
    -> Session
    kpset_id                        : int
    ---
    kpset_method                    : varchar(15)   # deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap
    kpset_config_path               : varchar(255)  # Path relative to root data directory where the config file is located
    kpset_videos_path               : varchar(255)  # Path relative to root data directory where the videos and their keypoints are located
    kpset_description=''            : varchar(300)  # Optional. User-entered description

    """

    class VideoFiles(dj.Part):
        """IDs and file paths of each video file.

        Atribbutes:
            video_id (int): Unique ID for each video.
            video_path (str): Filepath of each video, relative to root data directory.
        """

        definition = """
        -> master
        video_id                    : int
        ---
        video_path                  : varchar(255) # Filepath of each video, relative to root data directory
        """


@schema
class RecordingInfo(dj.Imported):
    """Automated table to store video metadata.

    Attributes:
        KeypointSet.VideoFiles (foreign key)    : Unique ID for each video.
        px_height (smallint)                    : Height in pixels.
        px_width (smallint)                     : Width in pixels.
        nframes (int)                           : Number of frames.
        fps (int)                               : Optional. Frames per second, Hz.
        recording_datetime (datetime)           : Optional. Datetime for the start of recording.
        recording_duration (float)              : Video duration (s) from nframes / fps.
    """

    definition = """
    -> KeypointSet.VideoFiles
    ---
    px_height                 : smallint  # Height in pixels
    px_width                  : smallint  # Width in pixels
    nframes                   : int       # Number of frames 
    fps = NULL                : int       # Optional. Frames per second, Hz
    recording_datetime = NULL : datetime  # Optional. Datetime for the start of the recording
    recording_duration        : float     # Video duration (s) from nframes / fps
    """

    @property
    def key_source(self):
        """Defines order of keys for the make function when called via `populate()`"""
        return KeypointSet & KeypointSet.VideoFiles

    def make(self, key):
        """
        Make function to populate the RecordingInfo table.

        Args:
            key (dict): Primary key from the RecordingInfo table.

        Returns:
            dict: Primary key and attributes for the RecordingInfo table.

        Raises:
        High-Level Logic:
        1. Fetches the file paths and video IDs from the KeypointSet.VideoFiles table.
        2. Iterates through the file paths and video IDs to obtain the video metadata using OpenCV.
        3. Inserts the video metadata into the RecordingInfo table.

        """

        file_paths, video_ids = (KeypointSet.VideoFiles & key).fetch(
            "video_path", "video_id"
        )

        for fp, video_id in zip(file_paths, video_ids):
            nframes = 0
            px_height, px_width, fps = None, None, None

            file_path = (find_full_path(get_kpms_root_data_dir(), fp)).as_posix()

            cap = cv2.VideoCapture(file_path)
            info = (
                int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                int(cap.get(cv2.CAP_PROP_FPS)),
            )
            if px_height is not None:
                assert (px_height, px_width, fps) == info
            px_height, px_width, fps = info
            nframes += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

            self.insert1(
                {
                    **key,
                    "video_id": video_id,
                    "px_height": px_height,
                    "px_width": px_width,
                    "nframes": nframes,
                    "fps": fps,
                    "recording_duration": nframes / fps,
                }
            )


@schema
class Bodyparts(dj.Manual):
    """Table for storing the bodyparts used in the analysis.

    Attributes:
        KeypointSet (foreign key)       : Unique ID for each keypoint set.
        bodyparts_id (int)              : Unique ID for each bodypart.
        anterior_bodyparts (longblob)   : List of strings of anterior bodyparts
        posterior_bodyparts (longblob)  : List of strings of posterior bodyparts
        use_bodyparts (longblob)        : List of strings of bodyparts to be used
    """

    definition = """
    -> KeypointSet
    bodyparts_id                : int
    ---
    anterior_bodyparts          : blob  # List of strings of anterior bodyparts
    posterior_bodyparts         : blob  # List of strings of posterior bodyparts
    use_bodyparts               : blob  # List of strings of bodyparts to be used
    """


@schema
class PCATask(dj.Manual):
    """
    Table to define the PCA task.

    Attributes:
        KeypointSet (foreign key)       : Unique ID for each keypoint set.
        Bodyparts (foreign key)         : Unique ID for each bodypart.
        pca_task_id (int)               : Unique ID for each PCA task.
        project_path (str)              : KPMS's project_path in config relative to root
        task_mode (str)                 : 'load': load computed analysis results, 'trigger': trigger computation
    """

    definition = """ 
    -> KeypointSet
    -> Bodyparts
    pca_task_id: int
    ---
    project_path='' : varchar(255)             # KPMS's project_path in config relative to root
    task_mode='load' : enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation
    """

@schema
class FormattedDataset(dj.Imported):
    """
    Table for storing the formatted dataset.
    """

    definition = """
    -> PCATask
    ---
    config                  : longblob # stored full config file
    coordinates             : longblob
    confidences             : longblob             
    bodyparts               : longblob
    data                    : longblob
    metadata                : longblob
    """

    def make(self, key):
        """
        Make function to format keypoint coordinates and confidences for inference.
        
        Args:
            key (dict): Primary key from the PCATask table.

        Returns:
            dict: Primary key and attributes for the PCATask table.

        Raises:
        
        High-Level Logic:
        
        """

        anterior_bodyparts, posterior_bodyparts, use_bodyparts = (
        Bodyparts & key
        ).fetch1(
            "anterior_bodyparts",
            "posterior_bodyparts",
            "use_bodyparts",
        )
        project_path = (PCATask & key).fetch1("project_path")
        task_mode = (PCATask & key).fetch1("task_mode")
        method = (KeypointSet & key).fetch1("kpset_method")
        format = (PoseEstimationMethod & {'format':method}).fetch1("format")
        kpset_config_path, kpset_videos_path = (KeypointSet & key).fetch1("kpset_config_path","kpset_videos_path")
            
        from keypoint_moseq import (setup_project,
                                    load_config,
                                    check_config_validity,
                                    update_config,
                                    load_keypoints,
                                    format_data)   
        
        if task_mode == "trigger":
            config = setup_project(
                    project_path, deeplabcut_config=kpset_config_path
                ) # setup a project directory for deeplabcut, sleap or nwb config, and generate a `config.yml` file with project settings. Overwrite by default is false. If the project dir already exists, pick a different project dir name. 


        # TO-DO: Here there should be the creation of a DJ_config file to update the new bodyparts
        elif task_mode == "load":
            config_kwargs_dict = dict(
            video_dir = kpset_videos_path,
            anterior_bodyparts = anterior_bodyparts,
            posterior_bodyparts = posterior_bodyparts,
            use_bodyparts = use_bodyparts)
            
        ## The following function `update_config` does the following: (1) config = load_config(...), (2) config.update(kwargs), (3) generate_config(project_dir, **config)
            update_config(
            project_path,
            **config_kwargs_dict
        )

        # To check load_config, this is the function:
        # check_if_valid = True
        # build_indexes = True
        # config_path = os.path.join(project_path, "config.yml")

        ## Proposal: Instead of updating the new bodyparts in the original kpms_config file, the previous function will be splitted to create a new config file instead named `dj_config.yml` with the new bodyparts
            # config=load_config(project_path, check_if_valid=False, build_indexes=False)
            # config.update(config_kwargs_dict)
            # generate a new config file, not overwriting the original config file


        # load data from deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap
        coordinates, confidences, bodyparts = load_keypoints(
            filepath_pattern=kpset_videos_path, format=format
        )
        
        # coordinates: dict
        #     Dictionary mapping filenames to keypoint coordinates as ndarrays of
        #     shape (n_frames, n_bodyparts, 2[or 3])

        # confidences: dict
        #     Dictionary mapping filenames to `likelihood` scores as ndarrays of
        #     shape (n_frames, n_bodyparts)

        # bodyparts: list of str
        #     List of bodypart names. The order of the names matches the order of the
        #     bodyparts in `coordinates` and `confidences`.

        # formatted_bodyparts is not necessarily the same as use_bodyparts



        # format data for modeling
        # Data are transformed as follows:
        #     1. Coordinates and confidences are each merged into a single array
        #        using :py:func:`keypoint_moseq.util.batch`. Each row of the merged
        #        arrays is a segment from one recording.
        #     2. The keypoints axis is reindexed according to the order of elements
        #        in `use_bodyparts` with respect to their initial orer in
        #        `bodyparts`.
        #     3. Uniform noise proportional to `added_noise_level` is added to the
        #        keypoint coordinates to prevent degenerate solutions during fitting.
        #     4. Keypoint confidences are augmented by `conf_pseudocount`.
        #     5. Wherever NaNs occur in the coordinates, they are replaced by values
        #        imputed using linear interpolation, and the corresponding
        #        confidences are set to `conf_pseudocount`.

        # TO-DO: fix issue with the config["anterior_idxs"] since they are iterated as characters, not words, and rise an error: ` 'n' is not in list`
        data, metadata = format_data(
            **config, coordinates=coordinates, confidences=confidences
        )
        # data: dict with the following items
        #     Y: jax array with shape (n_segs, seg_length, K, D)
        #         Keypoint coordinates from all recordings broken into fixed-length segments.

        #     conf: jax array with shape (n_segs, seg_length, K)
        #         Confidences from all recordings broken into fixed-length segments. If no input is provided for confidences, then data["conf"]=None.

        #     mask: jax array with shape (n_segs, seg_length)
        #         Binary array where 0 indicates areas of padding
        #         (see keypoint_moseq.util.batch).

        # metadata: tuple (keys, bounds)
        #     Metadata for the rows of Y, conf and mask, as a tuple with a array of recording names and an array of (start,end) times. See
        #     jax_moseq.utils.batch for details.


        # TO-DO: store data and metadata in files (Not allowed to store this jax data type as longblob in the table)

        self.FormattedDataset.insert1(
            dict(
                **key,
                config=config,
                coordinates=coordinates,
                confidences=confidences,
                bodyparts=bodyparts,
                # data=data, -----> should be saved as a file
                # metadata=metadata,-----> should be saved as a file
            )
        )


@schema
class PCAFitting(dj.Computed):
    definition = """
    -> FormattedDataset
    pca_fitting_id       : int 
    ---
    pca_fitting_time     : datetime  # Time of generation of the PCA fitting analysis 
    pca                  : longblob
    """

    def make(self, key):
        from keypoint_moseq import (
            load_pca,
            fit_pca,
            save_pca,
            print_dims_to_explain_variance,
            plot_scree,
            plot_pcs,
        )

        task_mode, project_path = (PCATask & key).fetch1("task_mode", "project_path")
        config = (PCATask.FormattedDataset & key).fetch1( "config")

        # data = ---> Load data from the file saved in the previous table
        
        project_path = find_full_path(get_kpms_root_data_dir(), project_path)

        if task_mode == "load":
            pca = load_pca(**data, **config())

        elif task_mode == "trigger":
            pca = fit_pca(**data, **config())
            save_pca(pca, project_path)
            creation_time = datetime.strftime("%Y-%m-%d %H:%M:%S")

        print_dims_to_explain_variance(pca, 0.9)
        plot_scree(pca, project_dir=project_path)
        plot_pcs(pca, project_dir=project_path, **config())

        self.insert1(**key, pca_fitting_time=creation_time, pca=pca)


@schema
class LatentDimension(dj.Lookup):
    definition = """
    latent_dim                : int
    ---
    latent_dim_description='' : varchar(1000)
    """


@schema
class UpdateLatentDimension(dj.Computed):
    definition = """
    -> PCAFitting
    -> LatentDimension
    """

    def make(self, key):
        # update latent_dim in config_file
        from keypoint_moseq import update_config

        project_path = (PCATask & key).fetch1("project_path")
        latent_dim = (LatentDimension & key).fetch1("latent_dim")
        update_config(project_path, latent_dim=latent_dim)
