from datetime import datetime
import inspect
import os
from pathlib import Path
import pickle
from typing import Optional
import importlib

import cv2
import datajoint as dj
import numpy as np

from element_interface.utils import find_full_path
from .readers.kpms_reader import generate_dj_config, load_dj_config
from keypoint_moseq import (
    setup_project,
    load_config,
    load_keypoints,
    format_data,
    load_pca,
    fit_pca,
    save_pca,
)


schema = dj.schema()
_linking_module = None


def activate(
    pca_schema_name: str,
    *,
    create_schema: bool = True,
    create_tables: bool = True,
    linking_module: str = None,
):
    """Activate this schema.

    Args:
        pca_schema_name (str): A string containing the name of the pca schema.
        create_schema (bool): If True (default), schema  will be created in the database.
        create_tables (bool): If True (default), tables related to the schema will be created in the database.
        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.

    Dependencies:
    Functions:
        get_kpms_root_data_dir(): Returns absolute path for root data director(y/ies) with all behavioral recordings, as (list of) string(s)
        get_kpms_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to session video subfolder.
    """

    if isinstance(linking_module, str):
        linking_module = importlib.import_module(linking_module)
    assert inspect.ismodule(
        linking_module
    ), "The argument 'dependency' must be a module's name or a module"

    assert hasattr(
        linking_module, "get_kpms_root_data_dir"
    ), "The linking module must specify a lookup function for a root data directory"

    global _linking_module
    _linking_module = linking_module

    # activate
    schema.activate(
        pca_schema_name,
        create_schema=create_schema,
        create_tables=create_tables,
        add_objects=_linking_module.__dict__,
    )


# -------------- Functions required by the element-moseq ---------------


def get_kpms_root_data_dir() -> list:
    """Pulls relevant func from parent namespace to specify root data dir(s).

    It is recommended that all paths in DataJoint Elements stored as relative
    paths, with respect to some user-configured "root" director(y/ies). The
    root(s) may vary between data modalities and user machines. Returns a full path
    string or list of strings for possible root data directories.
    """
    root_directories = _linking_module.get_kpms_root_data_dir()
    if isinstance(root_directories, (str, Path)):
        root_directories = [root_directories]

    if (
        hasattr(_linking_module, "get_kpms_processed_data_dir")
        and get_kpms_processed_data_dir() not in root_directories
    ):
        root_directories.append(_linking_module.get_kpms_processed_data_dir())

    return root_directories


def get_kpms_processed_data_dir() -> Optional[str]:
    """Pulls relevant func from parent namespace. Defaults to KPMS's project /videos/.

    Method in parent namespace should provide a string to a directory where KPMS output
    files will be stored. If unspecified, output files will be stored in the
    session directory 'videos' folder, per DeepLabCut default.
    """
    if hasattr(_linking_module, "get_kpms_processed_data_dir"):
        return _linking_module.get_kpms_processed_data_dir()
    else:
        return None


# ----------------------------- Table declarations ----------------------


@schema
class PoseEstimationMethod(dj.Lookup):
    """Table for storing the pose estimation method used to obtain the keypoints data.

    Attributes:
        format_method (str): Pose estimation method.
        pose_estimation_desc (str): Pose estimation method description.
    """

    definition = """ 
    # Parameters used to obtain the keypoints data based on a specific pose estimation method.        
    format_method           : char(15)         # deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap,
    ---
    pose_estimation_desc    : varchar(1000)    # Optional. Pose estimation method description
    """

    contents = [
        ["deeplabcut", "`.csv` and `.h5/.hdf5` files generated by DeepLabcut analysis"],
        ["sleap", "`.slp` and `.h5/.hdf5` files generated by SLEAP analysis"],
        ["anipose", "`.csv` files generated by anipose analysis"],
        ["sleap-anipose", "`.h5/.hdf5` files generated by sleap-anipose analysis"],
        ["nwb", "`.nwb` files with Neurodata Without Borders (NWB) format"],
        ["facemap", "`.h5` files generated by Facemap analysis"],
    ]


@schema
class KeypointSet(dj.Manual):
    """Table for storing the keypoint sets and their associated videos.

    Attributes:
        kpset_id (int): Unique ID for each keypoint set.
        PoseEstimationMethod (foreign key): Unique format method varchar used to obtain the keypoints data.
        kpset_config_dir (str): Path relative to root data directory where the config file is located.
        kpset_videos_dir (str): Path relative to root data directory where the videos and their keypoints are located.
        kpset_desc (str): Optional. User-entered description.
    """

    definition = """
    -> Session
    kpset_id                        : int
    ---
    -> PoseEstimationMethod
    kpset_config_dir                : varchar(255)  # Path relative to root data directory where the config file is located
    kpset_videos_dir                : varchar(255)  # Path relative to root data directory where the videos and their keypoints are located
    kpset_desc=''                   : varchar(300)  # Optional. User-entered description
    """

    class VideoFile(dj.Part):
        """IDs and file paths of each video file.

        Attributes:
            video_id (int): Unique ID for each video.
            video_path (str): Filepath of each video, relative to root data directory.
        """

        definition = """
        -> master
        video_id                    : int
        ---
        video_path                  : varchar(1000) # Filepath of each video, relative to root data directory
        """


@schema
class RecordingInfo(dj.Imported):
    """Automated table to store the average metadata of the videoset associated with a kpset_id.

    Attributes:
        KeypointSet (foreign key)               : Unique ID for each video set.
        px_height_average (smallint)            : Average height of the video set (pixels).
        px_width_average (smallint)             : Average width of the video set (pixels).
        nframes_average (int)                   : Average number of frames of the video set (frames).
        fps_average (int)                       : Optional. Average frames per second of the video set (Hz).
        recording_duration_average (float)      : Average video duration (s) of the video set (nframes / fps).
    """

    definition = """
    -> KeypointSet
    ---
    px_height_average                 : smallint  # Average height of the video set (pixels)
    px_width_average                  : smallint  # Average width of the video set (pixels)
    nframes_average                   : int       # Average number of frames of the video set (frames)
    fps_average = NULL                : int       # Optional. Average frames per second of the video set (Hz)
    recording_duration_average        : float     # Average video duration (s) of the video set (nframes / fps)
    """

    @property
    def key_source(self):
        """Defines order of keys for the make function when called via `populate()`"""
        return KeypointSet & KeypointSet.VideoFile

    def make(self, key):
        """
        Make function to populate the `RecordingInfo` table.

        Args:
            key (dict): A dictionary containing key information for the session

        Raises:

        High-Level Logic:
        1. Fetches the file paths and video IDs from the KeypointSet.VideoFiles table.
        2. Iterates through the file paths and video IDs to obtain the video metadata using OpenCV.
        3. Calculate the average video metadata and insert it into the RecordingInfo table.
        """

        file_paths, video_ids = (KeypointSet.VideoFile & key).fetch(
            "video_path", "video_id"
        )

        px_height_list = []
        px_width_list = []
        nframes_list = []
        fps_list = []
        recording_duration_list = []

        for fp, video_id in zip(file_paths, video_ids):
            file_path = (find_full_path(get_kpms_root_data_dir(), fp)).as_posix()

            cap = cv2.VideoCapture(file_path)
            px_height_list.append(int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))
            px_width_list.append(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))
            fps_list.append(int(cap.get(cv2.CAP_PROP_FPS)))
            nframes_list.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))
            cap.release()

        px_height_average = int(np.mean(px_height_list))
        px_width_average = int(np.mean(px_width_list))
        fps_average = int(np.mean(fps_list))
        nframes_average = int(np.mean(nframes_list))
        recording_duration_average = int(np.mean(nframes_list) / np.mean(fps_list))

        self.insert1(
            {
                **key,
                "px_height_average": px_height_average,
                "px_width_average": px_width_average,
                "nframes_average": nframes_average,
                "fps_average": fps_average,
                "recording_duration_average": recording_duration_average,
            }
        )


@schema
class Bodyparts(dj.Manual):
    """Table for storing the bodyparts used in the analysis.

    Attributes:
        KeypointSet (foreign key)       : Unique ID for each keypoint set.
        bodyparts_id (int)              : Unique ID for each bodypart.
        bodyparts_desc(varchar)         : Optional. User-entered description.
        anterior_bodyparts (blob)       : List of strings of anterior bodyparts
        posterior_bodyparts (blob)      : List of strings of posterior bodyparts
        use_bodyparts (blob)            : List of strings of bodyparts to be used
    """

    definition = """
    -> KeypointSet
    bodyparts_id                : int
    ---
    bodyparts_desc=''           : varchar(1000) # Optional. User-entered description.
    anterior_bodyparts          : blob  # List of strings of anterior bodyparts
    posterior_bodyparts         : blob  # List of strings of posterior bodyparts
    use_bodyparts               : blob  # List of strings of bodyparts to be used
    """


@schema
class PCATask(dj.Manual):
    """
    Table to define the PCA task.

    Attributes:
        Bodyparts (foreign key)         : Unique ID for each bodypart.
        output_dir (str)                : KPMS's output directory relative to root
        task_mode (str)                 : 'load': load computed analysis results, 'trigger': trigger computation
    """

    definition = """ 
    -> Bodyparts
    ---
    output_dir=''               : varchar(255) # KPMS's output directory relative to root
    task_mode='load'            : enum('load', 'trigger') # default = 'load': load computed analysis results, 'trigger': trigger computation
    """


@schema
class FormattedDataset(dj.Imported):
    """
    Table for storing the formatted dataset and update the `config.yml` by creating a new `dj_config.yml` in the project path (`output_dir`)

    Attributes:
        PCATask (foreign key)           : Unique ID for each PCATask.
        coordinates (longblob)          : Dictionary mapping filenames to keypoint coordinates as ndarrays of shape (n_frames, n_bodyparts, 2[or 3])
        confidences (longblob)          : Dictionary mapping filenames to `likelihood` scores as ndarrays of shape (n_frames, n_bodyparts)
        formatted_bodyparts (longblob)  : List of bodypart names. The order of the names matches the order of the bodyparts in `coordinates` and `confidences`.
    """

    definition = """
    -> PCATask
    ---
    coordinates             : longblob # Keypoint coordinates
    confidences             : longblob # Keypoint confidences                 
    formatted_bodyparts     : longblob #Formatted bodyparts
    """

    def make(self, key):
        """
        Make function to generate and update `dj_config.yml` and to format keypoint coordinates and confidences for inference.

        Args:
            key (dict): Primary key from the PCATask table.

        Raises:
            ValueError: If `task_mode` does not match either 'load' or 'trigger'

        High-Level Logic:
        1. Fetches the anterior_bodyparts, posterior_bodyparts, use_bodyparts, output_dir, and task_mode from the PCATask table.
        2. Fetches the format_method, kpset_config_dir, and kpset_videos_dir from the KeypointSet table.
        3. Based on the task_mode, it either triggers the computation or loads the computed analysis results.
        4. If task_mode='trigger', it creates an output_dir if it does not exist, creates a `dj_config` file with the default values from the pose estimation config, and update it with the video_dir and bodyparts used in the pipeline.
        6. If task_mode='load', it loads the `dj_config` file and updates it with the video_dir and bodyparts used in the pipeline.
        7. Then, it loads keypoint tracking results from one or more files and formats them for PCA analysis.
        """

        anterior_bodyparts, posterior_bodyparts, use_bodyparts = (
            Bodyparts & key
        ).fetch1(
            "anterior_bodyparts",
            "posterior_bodyparts",
            "use_bodyparts",
        )
        output_dir, task_mode = (PCATask & key).fetch1("output_dir", "task_mode")
        format_method, kpset_config_dir, kpset_videos_dir = (KeypointSet & key).fetch1(
            "format_method", "kpset_config_dir", "kpset_videos_dir"
        )

        if task_mode == "trigger":
            # create an output_dir if it does not exist, and create a config file with the default values from the pose estimation config
            setup_project(
                output_dir, deeplabcut_config=kpset_config_dir + "/config.yaml"
            )  # creates KPMS default config file from dlc data
            config = load_config(output_dir, check_if_valid=True, build_indexes=False)

            # update the config dict with the video_dir and bodyparts used in the pipeline
            config_kwargs_dict = dict(
                video_dir=kpset_videos_dir,
                anterior_bodyparts=anterior_bodyparts,
                posterior_bodyparts=posterior_bodyparts,
                use_bodyparts=use_bodyparts,
            )
            config.update(**config_kwargs_dict)

            # save the updated config dict to a different file named `dj_config.yml`
            generate_dj_config(output_dir, **config)

        elif task_mode == "load":
            config = load_dj_config(output_dir)

            # update the config dict with the video_dir and bodyparts used in the pipeline
            config_kwargs_dict = dict(
                video_dir=kpset_videos_dir,
                anterior_bodyparts=anterior_bodyparts,
                posterior_bodyparts=posterior_bodyparts,
                use_bodyparts=use_bodyparts,
            )
            config.update(**config_kwargs_dict)

            # update the updated config dict to the file `dj_config.yml`
            generate_dj_config(output_dir, **config)

        else:
            raise ValueError("task_mode should be either 'load' or 'trigger'")

        # load keypoints data from deeplabcut, sleap, anipose, sleap-anipose, nwb, facemap
        coordinates, confidences, formatted_bodyparts = load_keypoints(
            filepath_pattern=kpset_videos_dir, format=format_method
        )

        self.insert1(
            dict(
                **key,
                coordinates=coordinates,
                confidences=confidences,
                formatted_bodyparts=formatted_bodyparts,
            )
        )


@schema
class PCAFitting(dj.Computed):
    """Table for storing the PCA fitting analysis.

    Attributes:
        FormattedDataset (foreign key)  : Unique ID for FormattedDataset.
        pca_fitting_time (datetime)     : Time of generation of the PCA fitting analysis.
    """

    definition = """
    -> FormattedDataset
    ---
    pca_fitting_time=NULL    : datetime  # Time of generation of the PCA fitting analysis 
    """

    def make(self, key):
        """
        Make function to fit PCA model and save the PCA model, data, and metadata to files.
        
        Args:
            key (dict): Primary key from the FormattedDataset table.

        Raises:

        High-Level Logic:
        1. Fetches the task_mode and output_dir from the PCATask table.
        2. If task_mode='trigger', it loads the `dj_config` file and formats the keypoint coordinates and confidences for PCA analysis.
        3. Then, it fits the PCA model and saves the PCA model, data, and metadata to files.
        4. If task_mode='load', it does not perform any computation and sets the pca_fitting_time to NULL.
        5. Finally, it inserts the pca_fitting_time into the PCAFitting table.
        """
        task_mode, output_dir = (PCATask & key).fetch1("task_mode", "output_dir")

        if task_mode == "trigger":
            config = load_dj_config(output_dir, check_if_valid=True, build_indexes=True)
            coordinates, confidences = (FormattedDataset & key).fetch1(
                "coordinates", "confidences"
            )

            data, metadata = format_data(
                **config, coordinates=coordinates, confidences=confidences
            )

            pca = fit_pca(**data, **config)

            # save the pca model to a file
            save_pca(
                pca, output_dir
            )  # `pca.p` as the first pca model stored in the output_dir

            # save the data and metadata to files
            data_path = os.path.join(output_dir, "data.pkl")
            with open(data_path, "wb") as data_file:
                pickle.dump(data, data_file)

            metadata_path = os.path.join(output_dir, "metadata.pkl")
            with open(metadata_path, "wb") as metadata_file:
                pickle.dump(metadata, metadata_file)

            creation_time = datetime.utcnow()

        else:
            creation_time = None

        self.insert1(dict(**key, pca_fitting_time=creation_time))


@schema
class DimsExplainedVariance(dj.Computed):
    """
    This is an optional table to compute and store the latent dimensions that explain a certain specified variance threshold.

    Attributes:
        PCAFitting (foreign key)           : Unique ID for each PCAFitting.
        variance_percentage (float)        : Percentage of variance explained by the selected components.
        dims_explained_variance (int)      : Number of components required to explain the specified variance.
        latent_dim_desc (varchar)          : Description of the latent dimensions that explain the specified variance.
    """

    definition = """
    -> PCAFitting
    ---
    variance_percentage      : float # Percentage of variance explained by the selected components.
    dims_explained_variance  : int # Number of components required to explain the specified variance.
    latent_dim_desc          : varchar(1000) # Description of the latent dimensions that explain the specified variance.
    """

    def make(self, key):
        """
        Make function to compute and store the latent dimensions that explain a certain specified variance threshold.

        Args:
            key (dict): Primary key from the PCAFitting table.

        Raises:

        High-Level Logic:
        1. Fetches the output_dir from the PCATask table.
        2. Loads the PCA model from the output_dir.
        3. Computes the cumulative sum of the explained variance ratio and determines the number of components required to explain the specified variance.
        4. If the cumulative sum of the explained variance ratio is less than the specified variance threshold, it sets the dims_explained_variance to the number of components and variance_percentage to the cumulative sum of the explained variance ratio.
        5. If the cumulative sum of the explained variance ratio is greater than the specified variance threshold, it sets the dims_explained_variance to the number of components and variance_percentage to the specified variance threshold.
        6. Inserts the variance_percentage, dims_explained_variance, and latent_dim_desc into the DimsExplainedVariance table.
        """

        output_dir = (PCATask & key).fetch1("output_dir")
        variance_threshold = 0.90

        pca = load_pca(output_dir)
        cs = np.cumsum(pca.explained_variance_ratio_)
        # explained_variance_ratio_ndarray of shape (n_components,)
        # Percentage of variance explained by each of the selected components.
        # If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.
        if cs[-1] < variance_threshold:
            dims_explained_variance = len(cs)
            variance_percentage = cs[-1] * 100
            latent_dim_desc = (
                f"All components together only explain {cs[-1]*100}% of variance."
            )
        else:
            dims_explained_variance = (cs > variance_threshold).nonzero()[0].min() + 1
            variance_percentage = variance_threshold * 100
            latent_dim_desc = f">={variance_threshold*100}% of variance explained by {(cs>variance_threshold).nonzero()[0].min()+1} components."

        self.insert1(
            dict(
                **key,
                variance_percentage=variance_percentage,
                dims_explained_variance=dims_explained_variance,
                latent_dim_desc=latent_dim_desc,
            )
        )
