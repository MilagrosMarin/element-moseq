{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Element for Motion Sequencing with Keypoint-MoSeq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Open-source Data Pipeline for Motion Sequencing in Neurophysiology**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tutorial for the DataJoint Element for motion sequencing analysis. This tutorial aims to provide a comprehensive understanding of the open-source data pipeline by `element-moseq`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](../images/flowchart.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is designed to seamlessly integrate the **PCA fitting**, **model fitting** through **initialization**, **fitting an AR-HMM**, and **fitting the full keypoint-SLDS model** into a data pipeline and streamline model and video management using DataJoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](../images/pipeline.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this tutorial, you will have a clear grasp of how to set up and integrate the `Element MoSeq` into your specific research projects and your lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Please see the [datajoint tutorials GitHub repository](https://github.com/datajoint/datajoint-tutorials/tree/main) proceeding.\n",
    "A basic understanding of the following DataJoint concepts will be beneficial to your understanding of this tutorial:\n",
    "\n",
    "1. The `Imported` and `Computed` tables types in `datajoint-python`.\n",
    "2. The functionality of the `.populate()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tutorial Overview**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup\n",
    "- _Activate_ the DataJoint pipeline\n",
    "- _Insert_ example data into subject and session tables\n",
    "- _Insert_ the keypoint data from the pose estimation and the body parts in the DataJoint pipeline\n",
    "- _Fit a PCA model_ to aligned and centered keypoint coordinates and _select_ the latent dimension\n",
    "- _Train the AR-HMM and Keypoint-SLDS Models_\n",
    "- _Run the inference_ task and _visualize_ the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial loads the keypoint data extracted by DeepLabCut of a single freely moving mouse in an open-field environment. The open-source data is used as an example in the [Keypoint-MoSeq collab tutorial](https://colab.research.google.com/github/dattalab/keypoint-moseq/blob/main/docs/keypoint_moseq_colab.ipynb#scrollTo=e944d0e1).\n",
    "\n",
    "The goal is to link this point tracking to pose dynamics by identifying its behavioral modules (\"syllables\") without human supervision. The modeling results are stored as a `.h5` file and a subdirectory of `.csv` files that contain the following information:\n",
    "\n",
    "- Behavior modules as \"syllables\": the syllable label assigned to each frame (i.e. the state indexes assigned by the model)\n",
    "- Centroid and heading in each frame, as estimated by the model, that capture the animal's overall position in allocentric coordinates\n",
    "- Latent state: low-dimensional representation of the animal's pose in each frame. These are similar to PCA scores, and are modified to reflect the pose dynamics and noise estimates inferred by the model.\n",
    "\n",
    "The results of this Element example can be combined with **other modalities** to create a complete customizable data pipeline for your specific lab or study. For instance, you can combine `element-moseq` with `element-deeplabcut` and `element-calcium-imaging` to characterize the neural activity along with natural sub-second rhythmicity in mouse movement.\n",
    "\n",
    "#### Steps to Run the Element-MoSeq\n",
    "\n",
    "The input data for this data pipeline is as follows:\n",
    "\n",
    "- A DeepLabCut (DLC) project folder with its configuration file as `.yaml` file, video set as `.mp4`, and keypoint tracking as `.h5` files.\n",
    "- Selection of the anterior, posterior, and use bodyparts for the model fitting.\n",
    "\n",
    "This tutorial includes the keypoints example data in `example_data/inbox/dlc_project`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start this tutorial by importing the packages necessary to run the data pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from element_moseq.moseq_infer import get_kpms_processed_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tutorial is run in Codespaces, a private, local database server is created and made available for you. This is where we will insert and store our processed results.\n",
    "\n",
    "Let's connect to the database server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activate the DataJoint pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial presumes that the `element-moseq` has been pre-configured and instantiated, with the database linked downstream to pre-existing `subject` and `session` tables. Please refer to the `tutorial_pipeline.py` for the source code.\n",
    "\n",
    "Now, we will proceed to import the essential schemas required to construct this data pipeline, with particular attention to the primary components: `moseq_train` and `moseq_infer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import lab, subject, session, moseq_train, moseq_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the tables in the `moseq_train` and `moseq_infer` schemas as well as some of the upstream dependencies to `session` and `subject` schemas as a diagram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(moseq_train)\n",
    "    + dj.Diagram(moseq_infer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the diagram, this data pipeline encompasses several tables associated with different keypoint-MoSeq components like pca, pre-fitting of AR-HMM, and full fitting of the model. A few tables, such as `subject.Subject` or `session.Session`, while important for a complete pipeline, fall outside the scope of the `element-moseq` tutorial, and will therefore, not be explored extensively here. The primary focus of this tutorial will be on the `moseq_train` and `moseq_infer` schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(moseq_train) + dj.Diagram(moseq_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Insert example data into subject and session tables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delve into the `subject.Subject` and `session.Session` tables and include some example data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new entry for a subject in the `Subject` table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        sex=\"F\",\n",
    "        subject_birth_date=\"2024-01-01\",\n",
    "        subject_description=\"test subject\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create session keys and input them into the `Session` table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the dictionary named \"session_keys\"\n",
    "session_keys = [\n",
    "    dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\"),\n",
    "    dict(subject=\"subject1\", session_datetime=\"2024-03-16 14:43:10\"),\n",
    "]\n",
    "\n",
    "# Insert this dictionary in the Session table\n",
    "session.Session.insert(session_keys, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the inserted data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a `key` to use throughout the notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2024-03-15 14:04:22\")\n",
    "session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Insert the keypoint data from the pose estimation and the body parts in the DataJoint pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PoseEstimationMethod` table contains the pose estimation methods and file formats supported by the keypoint loader of `keypoint-moseq` package. In this tutorial, the keypoint input data are `.h5` files that have been obtained using `DeepLabCut`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.PoseEstimationMethod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keypoint input metadata into the `KeypointSet` table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.KeypointSet.insert1(\n",
    "    {\n",
    "        \"kpset_id\": 1,\n",
    "        \"pose_estimation_method\": \"deeplabcut\",\n",
    "        \"kpset_dir\": \"dlc_project\",\n",
    "        \"kpset_desc\": \"Example keypoint set\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.KeypointSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the video files in `KeypointSet.VideoFile` that will be used to fit the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_path = [\n",
    "    \"dlc_project/videos/21_12_10_def6a_3.top.ir.mp4\",\n",
    "    \"dlc_project/videos/22_04_26_cage4_1_1.top.ir.mp4\",\n",
    "    \"dlc_project/videos/21_12_10_def6a_1_1.top.ir.mp4\",\n",
    "    \"dlc_project/videos/22_27_04_cage4_mouse2_0.top.ir.mp4\",\n",
    "    \"dlc_project/videos/22_04_26_cage4_0.top.ir.mp4\",\n",
    "    \"dlc_project/videos/21_11_8_one_mouse.top.ir.Mp4\",\n",
    "    \"dlc_project/videos/21_12_2_def6b_2.top.ir.mp4\",\n",
    "    \"dlc_project/videos/21_12_10_def6b_3.top.ir.Mp4\",\n",
    "    \"dlc_project/videos/22_04_26_cage4_0_2.top.ir.mp4\",\n",
    "    \"dlc_project/videos/21_12_2_def6a_1.top.ir.mp4\",\n",
    "]\n",
    "\n",
    "# Insert the video files in the `VideoFile` table\n",
    "moseq_train.KeypointSet.VideoFile.insert(\n",
    "    (\n",
    "        {\"kpset_id\": 1, \"video_id\": v_idx, \"video_path\": f}\n",
    "        for v_idx, f in enumerate(videos_path)\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "moseq_train.KeypointSet.VideoFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's insert the body parts to use in the analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_task_key = {\"kpset_id\": 1, \"bodyparts_id\": 1}\n",
    "moseq_train.Bodyparts.insert1(\n",
    "    {\n",
    "        **pca_task_key,\n",
    "        \"anterior_bodyparts\": [\"nose\"],\n",
    "        \"posterior_bodyparts\": [\"spine4\"],\n",
    "        \"use_bodyparts\": [\n",
    "            \"spine4\",\n",
    "            \"spine3\",\n",
    "            \"spine2\",\n",
    "            \"spine1\",\n",
    "            \"head\",\n",
    "            \"nose\",\n",
    "            \"right ear\",\n",
    "            \"left ear\",\n",
    "        ],\n",
    "        \"bodyparts_desc\": \"Example of KPMS bodyparts extracted with DLC 2.3.9\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.Bodyparts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fit a PCA model to aligned and centered keypoint coordinates and select the latent dimension**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct model fitting for keypoint-MoSeq, both a PCA model and the latent dimension of the pose trajectory are necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(moseq_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PCATask` table serves the purpose of specifying the PCA task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCATask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and inserting a PCA task requires:\n",
    "\n",
    "1. Select a keypoint set\n",
    "2. Select the body parts to use\n",
    "3. Specify the output directory for the KPMS project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCATask.insert1(\n",
    "    {\n",
    "        **pca_task_key,\n",
    "        \"kpms_project_output_dir\": \"kpms_project_tutorial\",\n",
    "        \"task_mode\": \"load\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCATask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the PCA fitting, the keypoint detections and body parts need to be formatted. The resulting coordinates and confidences scores will be used to format the data for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCAPrep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the `PCAPrep` table will:\n",
    "\n",
    "1. Create the output directory, if it does not exist, with the kpms default `config.yml` file that contains the default values from the pose estimation\n",
    "2. Generate a copy as `dj_config.yml` and update it with both the video directory and the bodyparts\n",
    "3. Create and store the keypoint coordinates and confidences scores to format the data for the PCA fitting\n",
    "4. Calculate the average frame rate of the videoset chosen to train the model. This will be useful to calculate the kappa value in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCAPrep.populate(pca_task_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCAPrep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PCAFit` computation will format the aligned and centered keypoint coordinates, fit a PCA model, and save it as `pca.p` file in the output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCAFit.populate(pca_task_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PCAFit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still need to determine the specific dimension of the pose trajectory to utilize for fitting the keypoint-MoSeq model. A helpful guideline is to consider the number of dimensions required to explain 90% of the variance, or a maximum of 10 dimensions, whichever is lower.\n",
    "\n",
    "The computation of `LatentDimension` will automatically identify the components that explain 90% of the variance, aiding the user in making the final decision regarding an appropriate latent dimension for model fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.LatentDimension.populate(pca_task_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.LatentDimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid the user in selecting the latent dimensions for model fitting, two plots are created below: a cumulative scree plot and a visualization of each Principal Component (PC). In this visualization, translucent nodes/edges represent the mean pose, while opaque nodes/edges represent a perturbation in the direction of the PC.\n",
    "The plots are stored in the output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and store plots for the user to choose the latent dimensions in the next step\n",
    "from keypoint_moseq import load_pca, plot_scree, plot_pcs\n",
    "from element_moseq.readers.kpms_reader import load_kpms_dj_config\n",
    "from element_moseq.moseq_infer import get_kpms_processed_data_dir\n",
    "\n",
    "kpms_project_output_dir = (moseq_train.PCATask & pca_task_key).fetch1(\n",
    "    \"kpms_project_output_dir\"\n",
    ")\n",
    "kpms_project_output_dir = get_kpms_processed_data_dir() / kpms_project_output_dir\n",
    "\n",
    "kpms_dj_config = load_kpms_dj_config(\n",
    "    kpms_project_output_dir.as_posix(), check_if_valid=False, build_indexes=False\n",
    ")\n",
    "pca = load_pca(kpms_project_output_dir.as_posix())\n",
    "\n",
    "# plot_scree(pca, project_dir=kpms_project_output_dir.as_posix())\n",
    "# plot_pcs(pca, project_dir=kpms_project_output_dir.as_posix(), **kpms_dj_config)\n",
    "plot_scree(pca, savefig=False)\n",
    "plot_pcs(pca, savefig=False, **kpms_dj_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen dimension for the next steps in the analysis will be `latent dimension = 4`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train the AR-HMM and keypoint-SLDS Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-fitting and full-fitting processes for the KPMS Model involve the following steps:\n",
    "\n",
    "1. **Initialization**: Auto-regressive (AR) parameters and syllable sequences are randomly initialized using pose trajectories from PCA\n",
    "2. **Fitting an AR-HMM**: AR parameters, transition probabilities and syllable sequences are iteratively updated through Gibbs sampling\n",
    "3. **Fitting the full model**: All parameters, including both AR-HMM and centroid, heading, noise-estimates, and continuous latent states (i.e., pose trajectories) are iteratively updated through Gibbs sampling. This step is particularly useful for noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(moseq_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pre-fitting step (fitting an AR-HMM), a pre-fitting task needs to be defined and inserted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFitTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task requires the following inputs:\n",
    "\n",
    "1. The keypoint set, body parts, and latent dimension (extracted in the section above).\n",
    "2. A kappa value for the model pre-fitting.\n",
    "3. The number of iterations for the model pre-fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kappa hyperparameter**:\n",
    "An important decision for the user is to adjust the kappa hyperparameter to achieve the desired distribution of syllable durations. Higher values of kappa result in longer syllables.\n",
    "\n",
    "As a reference, let's choose a kappa value that yields a median syllable duration of 12 frames (400 ms), a duration recommended for rodents.\n",
    "\n",
    "During the model pre-fitting, it's advisable to explore different values of kappa (`kappa_range`) until the syllable durations stabilize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = (moseq_train.PCAPrep & pca_task_key).fetch1(\"average_frame_rate\")\n",
    "kappa_min = (12 / fps) * 1000  # ms\n",
    "kappa_max = 1e4  # ms\n",
    "kappa_range = np.logspace(np.log10(kappa_min), np.log10(kappa_max), num=3)\n",
    "kappa_range = np.round(kappa_range).astype(int)\n",
    "print([\"kappa = {:.2f} ms\".format(x) for x in kappa_range])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Iterations**: Typically, stabilizing the syllable duration requires 10-50 iterations during the model pre-fitting stage, while stabilizing the syllable sequence after setting kappa may take 200-500 iterations during the model full-fitting stage.\n",
    "\n",
    "We have already prepared one model with a `prefit_key` with `pre_latent_dim =4`, `pre_kappa=1e6` with `task_mode=trigger`.\n",
    "\n",
    "For tutorial purposes, we will use the **`task_mode = load`**, which will load the pre-fitted model located in the `outbox/kpms_project_tutorial`, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefit_key = {\n",
    "    **pca_task_key,\n",
    "    \"pre_latent_dim\": 4,\n",
    "    \"pre_kappa\": 1000000.0,\n",
    "    \"pre_num_iterations\": 5,\n",
    "    \"pre_fit_desc\": \"Tutorial PreFit task\",\n",
    "    \"task_mode\": \"load\",\n",
    "    \"model_name\": \"2024_03_28-18_14_26\",\n",
    "}\n",
    "prefit_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we will insert different entries (`prefit_keys`) in the `PreFitTask` with various kappa values until the target syllable time-scale is achieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFitTask.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFitTask.insert1(prefit_key, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the contents of the `PreFittingTask` table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFitTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When populating the `PreFit` table, the fitting of different AR-HMM models for each kappa defined in the `PreFitTask` will be automatically computed. This step will take a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFit.populate(prefit_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.PreFit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a `FullFitTask` based on the selected `latent_dimension = 4`, the chosen `kappa = 10000` based on the previous exploration.\n",
    "\n",
    "Again and for tutorial purposes, we will **`load`** a model already generated to ensure a smooth run of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.FullFitTask.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify kappa to maintain the desired syllable time-scale\n",
    "full_fit_key_1 = {\n",
    "    **pca_task_key,\n",
    "    \"full_latent_dim\": 4,\n",
    "    \"full_kappa\": 10000.0,\n",
    "    \"full_num_iterations\": 25,\n",
    "    \"full_fit_desc\": \"Fitting task with kappa = 10000 ms\",\n",
    "    \"task_mode\": \"load\",\n",
    "    \"model_name\": \"2024_03_28-18_54_08\",\n",
    "}\n",
    "\n",
    "moseq_train.FullFitTask.insert1(full_fit_key_1, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a second FullFitting task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_fit_key_2 = {\n",
    "    **pca_task_key,\n",
    "    \"full_latent_dim\": 4,\n",
    "    \"full_kappa\": 5000.0,\n",
    "    \"full_num_iterations\": 25,\n",
    "    \"full_fit_desc\": \"Fitting task with kappa = 5000 ms\",\n",
    "    \"task_mode\": \"load\",\n",
    "    \"model_name\": \"2024_03_28-18_15_54\",\n",
    "}\n",
    "\n",
    "moseq_train.FullFitTask.insert1(full_fit_key_2, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.FullFitTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.FullFit.populate([full_fit_key_1, full_fit_key_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_train.FullFit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run the inference task and visualize the results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models, along with their relevant information, will be registered in the DataJoint pipeline as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, latent_dim, kappa = (moseq_train.FullFit & \"full_kappa = 10000.\").fetch1(\n",
    "    \"model_name\", \"full_latent_dim\", \"full_kappa\"\n",
    ")\n",
    "moseq_infer.Model.insert1(\n",
    "    {\n",
    "        \"model_id\": 1,\n",
    "        \"model_name\": \"model 1\",\n",
    "        \"model_dir\": model_name,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"kappa\": kappa,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, latent_dim, kappa = (moseq_train.FullFit & \"full_kappa = 5000.\").fetch1(\n",
    "    \"model_name\", \"full_latent_dim\", \"full_kappa\"\n",
    ")\n",
    "moseq_infer.Model.insert1(\n",
    "    {\n",
    "        \"model_id\": 2,\n",
    "        \"model_name\": \"model 2\",\n",
    "        \"model_dir\": model_name,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"kappa\": kappa,\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the `Model` table to confirm that the two models have been registered:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Model comparison to select a model\n",
    "\n",
    "The expected marginal likelihood (EML) score can be used to rank models. The model with the highest EML score can then be selected for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = (moseq_train.FullFit).fetch(\"model_name\")\n",
    "\n",
    "checkpoint_paths = []\n",
    "for model_name in model_names:\n",
    "    checkpoint_paths.append(\n",
    "        get_kpms_processed_data_dir() / Path(model_name) / \"checkpoint.h5\"\n",
    "    )\n",
    "checkpoint_paths\n",
    "\n",
    "from keypoint_moseq import expected_marginal_likelihoods, plot_eml_scores\n",
    "\n",
    "eml_scores, eml_std_errs = expected_marginal_likelihoods(\n",
    "    checkpoint_paths=checkpoint_paths\n",
    ")\n",
    "best_model = model_names[np.argmax(eml_scores)]\n",
    "print(f\"Best model: {best_model}\")\n",
    "\n",
    "plot_eml_scores(eml_scores, eml_std_errs, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we choose the best ranked model for the inference task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_id = (moseq_infer.Model & \"model_dir = '{}'\".format(best_model)).fetch1(\n",
    "    \"model_id\"\n",
    ")\n",
    "print(f\"Best model id: {best_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tutorial purposes, we'll utilize the same video set (`videos_path`) employed for modeling training as the video set for inference. This will be incorporated into the `VideoRecording` table as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_key = {\n",
    "    **session_key,\n",
    "    \"recording_id\": 1,\n",
    "}\n",
    "moseq_infer.VideoRecording.insert1(\n",
    "    {**recording_key, \"device\": \"Camera1\"}, skip_duplicates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, video_name in enumerate(videos_path):\n",
    "    moseq_infer.VideoRecording.File.insert1(\n",
    "        dict(**recording_key, file_id=idx, file_path=video_name), skip_duplicates=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.VideoRecording * moseq_infer.VideoRecording.File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `InferenceTask` table serves the purpose of specifying an inference task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.InferenceTask.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and inserting a inference task requires:\n",
    "\n",
    "1. Define the subject and session datetime\n",
    "2. Define the video recording\n",
    "3. Define the pose estimation method used for the video recording\n",
    "4. Choose a model\n",
    "5. Specify the output directory and any optional parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_task = {**recording_key, \"model_id\": best_model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.InferenceTask.insert1(\n",
    "    {\n",
    "        **inference_task,\n",
    "        \"pose_estimation_method\": \"deeplabcut\",\n",
    "        \"keypointset_dir\": \"dlc_project/videos\",\n",
    "        \"inference_output_dir\": \"inference_output\",\n",
    "        \"inference_desc\": \"Inference task for the tutorial\",\n",
    "        \"num_iterations\": 5,  # Limited iterations for tutorial purposes.\n",
    "    },\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.InferenceTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating the `Inference` table will automatically extract learned states of the model (syllables, latent_state, centroid, and heading) and stored in the inference output directory together with visualizations and grid movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.Inference.populate(inference_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.Inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MotionSequence` table contains the results for the inference (syllables, latent_state, centroid, and heading):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.Inference.MotionSequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridMoviesSampledInstances` table contains the sampled instances for the grid movies. The sampled instances is a dictionary mapping syllables to lists of instances shown in each grid movie (in row-major order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moseq_infer.Inference.GridMoviesSampledInstances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_syllable_0 = (\n",
    "    moseq_infer.Inference.GridMoviesSampledInstances & \"syllable = 0\"\n",
    ").fetch1(\"instances\")\n",
    "instance_syllable_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance for syllable 0 is represented as a tuple containing the video name, start frame, and end frame. This format facilitates downstream analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from element_interface.utils import find_full_path\n",
    "\n",
    "syllable_id = 0\n",
    "\n",
    "model_dir = (moseq_infer.Model & inference_task).fetch1(\"model_dir\")\n",
    "inference_output_dir = (\n",
    "    moseq_infer.InferenceTask * moseq_infer.Inference.MotionSequence & inference_task\n",
    ").fetch(\"inference_output_dir\", limit=1)[0]\n",
    "model_path = find_full_path(get_kpms_processed_data_dir(), model_dir)\n",
    "video_path = (\n",
    "    model_path\n",
    "    / inference_output_dir\n",
    "    / \"grid_movies\"\n",
    "    / (\"syllable\" + str(syllable_id) + \".mp4\")\n",
    ").as_posix()\n",
    "print(video_path)\n",
    "gif_path = (\n",
    "    model_path\n",
    "    / inference_output_dir\n",
    "    / \"trajectory_plots\"\n",
    "    / (\"Syllable\" + str(syllable_id) + \".gif\")\n",
    ").as_posix()\n",
    "gif_path1 = (\n",
    "    model_path\n",
    "    / inference_output_dir\n",
    "    / \"trajectory_plots\"\n",
    "    / (\"Syllable\" + str(syllable_id + 1) + \".gif\")\n",
    ").as_posix()\n",
    "gif_path2 = (\n",
    "    model_path\n",
    "    / inference_output_dir\n",
    "    / \"trajectory_plots\"\n",
    "    / (\"Syllable\" + str(syllable_id + 2) + \".gif\")\n",
    ").as_posix()\n",
    "video_widget = widgets.Video.from_file(video_path, format=\"mp4\", width=640, height=480)\n",
    "display(video_widget)\n",
    "display(Image(filename=gif_path))\n",
    "display(Image(filename=gif_path1))\n",
    "display(Image(filename=gif_path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this tutorial, we have:\n",
    "\n",
    "- Covered the essential functionality of `element-moseq`\n",
    "- Acquired the skills to load the keypoint data and insert metadata into the pipeline\n",
    "- Learned how to fit a PCA, run the AR-HMM fitting and the Keypoint-SLDS fitting\n",
    "- Executed and ingested results of the motion sequencing analysis with Keypoint-MoSeq\n",
    "- Visualized and stored the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation and DataJoint tutorials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Detailed [documentation on `element-moseq`](https://datajoint.com/docs/elements/element-moseq/0.1/)\n",
    "- [General `DataJoint-Python` interactive tutorials](https://github.com/datajoint/datajoint-tutorials), covering fundamentals, such as table tiers, query operations, fetch operations, automated computations with the make function, and more.\n",
    "- [Documentation for `DataJoint-Python`](https://datajoint.com/docs/core/datajoint-python/0.14/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpms_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
